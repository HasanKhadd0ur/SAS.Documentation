"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[947],{6253:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"scraping-agent/architecture","title":"Architecture of Scraping Agent","description":"Layered Structure","source":"@site/docs/scraping-agent/architecture.md","sourceDirName":"scraping-agent","slug":"/scraping-agent/architecture","permalink":"/SAS.Documentation/docs/scraping-agent/architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/scraping-agent/architecture.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Scraping Agent","permalink":"/SAS.Documentation/docs/scraping-agent/"},"next":{"title":"Design Patterns in Scraping Agent","permalink":"/SAS.Documentation/docs/scraping-agent/design-patterns"}}');var s=r(4848),t=r(8453);const a={},c="Architecture of Scraping Agent",l={},o=[{value:"Layered Structure",id:"layered-structure",level:2},{value:"1. Core Layer",id:"1-core-layer",level:3},{value:"2. Scrapers Layer",id:"2-scrapers-layer",level:3},{value:"3. Pipeline Layer",id:"3-pipeline-layer",level:3},{value:"4. Kafka Layer",id:"4-kafka-layer",level:3},{value:"5. MongoDB Integration",id:"5-mongodb-integration",level:3},{value:"Flow Overview",id:"flow-overview",level:2}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"architecture-of-scraping-agent",children:"Architecture of Scraping Agent"})}),"\n",(0,s.jsx)(n.h2,{id:"layered-structure",children:"Layered Structure"}),"\n",(0,s.jsx)(n.p,{children:"The Scraping Agent is organized into five main layers, each with a distinct responsibility:"}),"\n",(0,s.jsx)(n.h3,{id:"1-core-layer",children:"1. Core Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Shared services (NER, logger, configuration loader)"}),"\n",(0,s.jsx)(n.li,{children:"Abstract interfaces"}),"\n",(0,s.jsx)(n.li,{children:"Global error handling"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-scrapers-layer",children:"2. Scrapers Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Platform-specific scraper implementations (e.g., Telegram, Twitter)"}),"\n",(0,s.jsx)(n.li,{children:"Base class defines a common interface for all scrapers"}),"\n",(0,s.jsx)(n.li,{children:"Factory class dynamically instantiates scraper by task"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-pipeline-layer",children:"3. Pipeline Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Processes raw messages through pluggable stages:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Keyword filtering"}),"\n",(0,s.jsx)(n.li,{children:"Hate/Spam detection"}),"\n",(0,s.jsx)(n.li,{children:"Sentiment classification"}),"\n",(0,s.jsx)(n.li,{children:"Named Entity Recognition"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Each stage is modular and reusable"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-kafka-layer",children:"4. Kafka Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Sends processed messages to appropriate Kafka topics"}),"\n",(0,s.jsx)(n.li,{children:"Sends metadata or high-frequency entities to side channels"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"5-mongodb-integration",children:"5. MongoDB Integration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fetches runtime configurations and credentials securely"}),"\n",(0,s.jsx)(n.li,{children:"Supports per-platform settings (e.g., Telegram credentials, search keywords)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"flow-overview",children:"Flow Overview"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Received"}),": A scraping task is fetched from Scraping Management Service"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scraper Instantiated"}),": The appropriate scraper is selected via a factory"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Messages Collected"}),": API or crawler retrieves raw messages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pipeline Executed"}),": Each message passes through the pipeline stages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Message Published"}),": Enriched output is sent to Kafka for further processing"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>c});var i=r(6540);const s={},t=i.createContext(s);function a(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);
"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[171],{4066:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>g,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"scraping-agent/index","title":"Scraping Agent","description":"The Scraping Agent is a Python-based microservice responsible for retrieving raw messages from various social media platforms like Telegram and Twitter. It supports both API-based and crawler-based extraction mechanisms.","source":"@site/docs/scraping-agent/index.md","sourceDirName":"scraping-agent","slug":"/scraping-agent/","permalink":"/SAS.Documentation/docs/scraping-agent/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/scraping-agent/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Frequently Asked Questions (FAQ)","permalink":"/SAS.Documentation/docs/faq"},"next":{"title":"Architecture of Scraping Agent","permalink":"/SAS.Documentation/docs/scraping-agent/architecture"}}');var t=s(4848),r=s(8453);const a={},c="Scraping Agent",o={},l=[{value:"Key Responsibilities",id:"key-responsibilities",level:2},{value:"Technologies Used",id:"technologies-used",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"scraping-agent",children:"Scraping Agent"})}),"\n",(0,t.jsx)(n.p,{children:"The Scraping Agent is a Python-based microservice responsible for retrieving raw messages from various social media platforms like Telegram and Twitter. It supports both API-based and crawler-based extraction mechanisms."}),"\n",(0,t.jsx)(n.p,{children:"This service acts as the first stage in the data processing pipeline, fetching messages in real-time or periodically, then performing initial cleaning, entity extraction, and enrichment before passing them downstream via Kafka."}),"\n",(0,t.jsx)(n.h2,{id:"key-responsibilities",children:"Key Responsibilities"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Executing scraping tasks (via APIs or crawlers)"}),"\n",(0,t.jsx)(n.li,{children:"Preprocessing and enriching collected messages"}),"\n",(0,t.jsx)(n.li,{children:"Publishing structured data to Kafka topics"}),"\n",(0,t.jsx)(n.li,{children:"Managing scraper configurations and credentials via MongoDB"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"technologies-used",children:"Technologies Used"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language"}),": Python"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Framework"}),": Flask"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Queue"}),": Apache Kafka"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Database"}),": MongoDB"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"NER"}),": HuggingFace models (e.g., mBERT)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"alt text",src:s(6752).A+"",width:"975",height:"202"})})]})}function g(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},6752:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/image-684af6745948de5434304f899ee1f431.png"},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>c});var i=s(6540);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);